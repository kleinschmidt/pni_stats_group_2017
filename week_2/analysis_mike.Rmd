---
title: "Markus Analysis"
output: html_notebook
---

First, we load the data! 

```{r}
library(data.table)
library(ggplot2)
d <- fread('../data/batch2_33final.csv', na.strings="NaN")
colnames(d) <- c("RT","log_RT","ID", "trial", "featureCorrelation", "TaskTransition", "congruency", "correct", "task", "Miniblock_all", "Miniblock_n")
summary(d)
```
Convert things to factors that are factors: 

```{r}
d[,TaskTransition:=as.factor(TaskTransition)]
levels(d$TaskTransition) <- c("NoTaskTransition","TaskTransition")
contrasts(d$TaskTransition) <- c(-0.5, 0.5)

d[,featureCorrelation:=as.factor(featureCorrelation)]
levels(d$featureCorrelation) <- c("Uncorrelated","Correlated")
contrasts(d$featureCorrelation) <- c(-0.5, 0.5)

d[,congruency:=as.factor(congruency)]
levels(d$congruency) <- c("Incongruent","Congruent")
contrasts(d$congruency) <- c(-0.5, 0.5)
```

# Figures 

First thing we want to do, is sanity check our data. How about a histogram? 

```{r}
hist(d$RT)
```

Okay, this is very odd. We are seeing a spike of RTs right around 0. This should encourage us to look at the experiment code to verify there are no bugs. It could be that these are anticipatory responses, though I would expect those to be less of a spike right at 0. We can see whether all subjects show this: 

```{r}
p <- ggplot(d, aes(x=log(RT))) + geom_histogram() + facet_wrap(~ID)
print(p)
```
Ok, so very clearly this is not all subjects. We have subject 7 the occasional early guesser and subject 22 who really wanted to get out of the experiment. Is that right? We can verify by looking at the accuracy? 

```{r}
d[ID==22,mean(correct, na.rm=T)]
d[ID==7,mean(correct, na.rm=T)]
d[!(ID  %in% c(7,22)),mean(correct, na.rm=T)]
```
Either everyone has accuracy below chance, or 'correct' actually codes errors. Fix that now: 

```{r}
library(plyr)
d[,correct:=mapvalues(correct,from=c(0,1), to=c(1,0))]
d[ID==22,mean(correct, na.rm=T)]
d[ID==7,mean(correct, na.rm=T)]
d[!(ID  %in% c(7,22)),mean(correct, na.rm=T)]
meanAccBySubj <- d[,mean(correct,na.rm=T),by="ID"]
sd(meanAccBySubj$V1)
```
Ok, so subject 22 is at chance performance (consistent with jamming the button to get out of the experiment as quickly as possible). Subject 7 is an occasional guesser but accuracy is sane, so probably worth keeping. This is also the sort of thing that is worth going back to one's experiment notes for (did you notice this when you were running the subject?). For now, remove him/her: 

```{r}
d <- d[ID!=22]
hist(d$RT)
hist(d$log_RT)
```

Much better. This illustrates part of the reason that outlier removal is such a black art: if we did a cutoff on the whole population we would still keep higher RTs from this guessing subject, and remove fine RTs from other subjects. In the paper I wouldsay something like"one subject was excluded from analysis for failing to achieve better than chance performance". 

At this point, one could also add a hard cutoff on the minimum RT. The important thing here is that guesses can happen at any time, so cutting off too aggressively can be problematic. A cutoff of 50ms will remove less than 1% of data, maybe okay here. 

```{r}
nrow(d[RT<0.05])/nrow(d)
d <- d[RT>0.05]
```

Now we can plot some of our contrasts of interest. Since there is between-subject variation of within-subject effects, we don't want to just plot means and SEs computed from treating the whole dataset is i.i.d. But if we wanted to, here is one way to do this: 

```{r}
p <- ggplot(d, aes(x=TaskTransition, y=log_RT))  + stat_summary(fun.data="mean_cl_boot") 
p
```

We can do this for the inputs we care about. One way to do this with ggplot would be to reshape the data just right and use facet_wrap with scale=free. But reshape has never made sense to me, so here is an alternative: 

```{r}
library(gridExtra)
d[,miniblockIsZero:=(Miniblock_all==0)]
p_taskTransition_naive <- ggplot(d, aes(x=TaskTransition, y=log_RT)) + stat_summary(fun.data="mean_cl_normal") + facet_grid(~miniblockIsZero, labeller="label_both")
p_congruency_naive <- ggplot(d, aes(x=congruency, y=log_RT)) + stat_summary(fun.data="mean_cl_normal") + facet_grid(~miniblockIsZero, labeller="label_both")
p_featureCorrelation_naive <- ggplot(d, aes(x=featureCorrelation, y=log_RT)) + stat_summary(fun.data="mean_cl_normal") + facet_grid(~miniblockIsZero, labeller="label_both")
grid.arrange(p_taskTransition_naive, p_congruency_naive, p_featureCorrelation_naive, ncol=1)
```

Now, a better way of making this figure would be to somehow take out between-subject variability. One way would be to use some ideas from ANOVA theory for this, using a method from Morey 2008: 

```{r}
library(Rmisc)
library(plyr)
predictors <- c('TaskTransition','congruency','featureCorrelation')
d_summarized <- llply(predictors, function(x) summarySEwithin(data=as.data.frame(d), measurevar='log_RT', withinvars=c(x,'miniblockIsZero'), idvar='ID', na.rm=TRUE))

names(d_summarized) <- c('TaskTransition','congruency','featureCorrelation')

figurelist <- llply(predictors, function(x) ggplot(d_summarized[[x]], aes_string(x=x, y="log_RT")) + geom_pointrange(aes(ymin=log_RT-se, ymax=log_RT+se)) + facet_grid(~miniblockIsZero, labeller="label_both"))
grid.arrange(figurelist[[1]], figurelist[[2]], figurelist[[3]], nrow=1)
```

Now for stats. First we do the naive thing of a single regression for all the subjects: 

```{r}
mNaiveRegression <- lm(log_RT ~ trial + TaskTransition + congruency + featureCorrelation + TaskTransition:congruency + TaskTransition:featureCorrelation + congruency:featureCorrelation, data=d)
summary(mNaiveRegression)
library(stargazer)
stargazer(mNaiveRegression, type='text')
```

If we inspect residuals... 
```{r}
mNaiveRegression_untransformed <- lm(RT ~ trial + TaskTransition + congruency + featureCorrelation + TaskTransition:congruency + TaskTransition:featureCorrelation + congruency:featureCorrelation, data=d)
qqPlot(resid(mNaiveRegression_untransformed))
qqPlot(resid(mNaiveRegression))
# plot(resid(mNaiveRegression))
```

This should not be surprising: we are fitting some subjects better, and some worse. Since the data appears sorted by subject, we can add separator lines and we will see that the residuals are off in a subject-specific way: 

```{r}
newSubjIdx <- which(d$ID[1:(nrow(d)-1)] != d$ID[-1])
plot(resid(mNaiveRegression))
l_ply(newSubjIdx, function(x) abline(v=x))
```

So this was one giant subject. What about if we did the regressions separate for each subject? 

```{r}
subjIDs <- unique(d$ID)
bySubjectRegressions <- llply(subjIDs, function(x) lm(log_RT ~ trial + TaskTransition + congruency + featureCorrelation + TaskTransition:congruency + TaskTransition:featureCorrelation + congruency:featureCorrelation, data=d[ID==x]))
names(bySubjectRegressions) <- subjIDs
subjectCoefs <- llply(bySubjectRegressions, function(x) as.data.table(t(coef(x))))
for (x in subjIDs){
  subjectCoefs[[x]]$ID <- x
}

subjBetas <- rbindlist(subjectCoefs)
tests <- colnames(subjBetas)[2:7]
laply(tests, function(x) t.test(subjBetas[[x]])$p.value)
```

Now, proper linear mixed models. First we just do a model with different intercepts by subject:

```{r}
library(lme4)
lmmInterceptsOnly <- lmer(log_RT ~ trial + TaskTransition + congruency + featureCorrelation + TaskTransition:congruency + TaskTransition:featureCorrelation + congruency:featureCorrelation + (1|ID), data=d)
print(summary(lmmInterceptsOnly), corr=F)
plot(residuals(lmmInterceptsOnly))
```

Confidence intervals and p values get challenging here because the degrees of freedom are not defined. Instead, we can make an asymptotic gaussian approximation, use an approximation for the degrees of freedom (Satterhwaite or Kenward-Roger), or bootstrap: 

```{r}
confint(lmmInterceptsOnly, method='Wald') # asymptotic gaussian, least conservative
confint(lmmInterceptsOnly, method='boot') # bootstrapped
```

In this model, we assume subjects have different baselines, but the same coefficients. But maybe our subjects respond differently to our manipulation? We can try to estimate a model for subject-specific perturbations of all the slopes:

```{r}
lmmSlopes <- lmer(log_RT ~ trial + TaskTransition + congruency + featureCorrelation + TaskTransition:congruency + TaskTransition:featureCorrelation + congruency:featureCorrelation + (1+TaskTransition + congruency + featureCorrelation + TaskTransition:congruency + TaskTransition:featureCorrelation + congruency:featureCorrelation|ID), data=d)
print(summary(lmmSlopes), corr=F)
plot(residuals(lmmSlopes))
confint(lmmSlopes, method='Wald')
```

The difference between this and the intercepts-only model is telling us that some subjects are driving our effects by themselves. 

```{r}
subjPerturbations <- ranef(lmmSlopes)$ID
for (effect in colnames(subjPerturbations[2:7])){
  hist(subjPerturbations[[effect]], main=effect)
}
```

Of course, these frequentist statistics are making claims like "across N replications of the experiment, if the population mean is 0, here is how frequently we expect to see this sample mean." But this is not what I care about. First, because the null of "effect is 0.0001" isn't that much more interesting than the null of "effect is zero" -- zero is not special. Second, because I want to make claims like "in the true population, the effect size is at least X with probability Y". That brings us to Bayesian inference: 

```{r}
library(rstanarm)
options(mc.cores = parallel::detectCores())
lmmSlopesBayesian <- stan_lmer(log_RT ~ trial + TaskTransition + congruency + featureCorrelation + TaskTransition:congruency + TaskTransition:featureCorrelation + congruency:featureCorrelation + (1+TaskTransition + congruency + featureCorrelation + TaskTransition:congruency + TaskTransition:featureCorrelation + congruency:featureCorrelation|ID), data=d, QR=TRUE, iter=2000, chains=6)
summary(lmmSlopesBayesian, pars='beta')
plot(lmmSlopesBayesian, pars='beta')
pp_check(lmmSlopesBayesian, nreps=20)
```
